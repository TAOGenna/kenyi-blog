<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on Kenyi&#39;Log</title>
    <link>https://taogenna.github.io/kenyi-blog/categories/ai/</link>
    <description>Recent content in AI on Kenyi&#39;Log</description>
    <generator>Hugo -- 0.147.9</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Oct 2025 12:20:23 -0500</lastBuildDate>
    <atom:link href="https://taogenna.github.io/kenyi-blog/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PPCA: The Minimal Generative Model</title>
      <link>https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/</link>
      <pubDate>Mon, 20 Oct 2025 12:20:23 -0500</pubDate>
      <guid>https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/</guid>
      <description>&lt;h1 id=&#34;ppca-probabilistic-principal-component-analysis&#34;&gt;PPCA: Probabilistic Principal Component Analysis&lt;/h1&gt;
&lt;p&gt;PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA &lt;em&gt;feels&lt;/em&gt; algebraic â€” where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the &lt;strong&gt;maximum-likelihood solution of a simple linear Gaussian latent-variable model&lt;/strong&gt;. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear + Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
