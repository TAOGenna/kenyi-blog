<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>PPCA: The Minimal Generative Model | Kenyi&#39;Log</title>
<meta name="keywords" content="">
<meta name="description" content="PPCA: Probabilistic Principal Component Analysis
PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA feels algebraic — where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the maximum-likelihood solution of a simple linear Gaussian latent-variable model. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear &#43; Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.">
<meta name="author" content="Kenyi Takagui-Perez">
<link rel="canonical" href="https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/">
<link crossorigin="anonymous" href="/kenyi-blog/assets/css/stylesheet.97ce433988149984f534c928fc914072e36e0e399108c98380f083ba885d9de0.css" integrity="sha256-l85DOYgUmYT1NMko/JFAcuNuDjmRCMmDgPCDuohdneA=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://taogenna.github.io/kenyi-blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://taogenna.github.io/kenyi-blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://taogenna.github.io/kenyi-blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://taogenna.github.io/kenyi-blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://taogenna.github.io/kenyi-blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>
<meta property="og:url" content="https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/">
  <meta property="og:site_name" content="Kenyi&#39;Log">
  <meta property="og:title" content="PPCA: The Minimal Generative Model">
  <meta property="og:description" content="PPCA: Probabilistic Principal Component Analysis PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA feels algebraic — where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the maximum-likelihood solution of a simple linear Gaussian latent-variable model. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear &#43; Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2025-10-20T12:20:23-05:00">
    <meta property="article:modified_time" content="2025-10-20T12:20:23-05:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PPCA: The Minimal Generative Model">
<meta name="twitter:description" content="PPCA: Probabilistic Principal Component Analysis
PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA feels algebraic — where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the maximum-likelihood solution of a simple linear Gaussian latent-variable model. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear &#43; Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Docs",
      "item": "https://taogenna.github.io/kenyi-blog/docs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "PPCA: The Minimal Generative Model",
      "item": "https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "PPCA: The Minimal Generative Model",
  "name": "PPCA: The Minimal Generative Model",
  "description": "PPCA: Probabilistic Principal Component Analysis PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA feels algebraic — where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the maximum-likelihood solution of a simple linear Gaussian latent-variable model. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear + Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.\n",
  "keywords": [
    
  ],
  "articleBody": "PPCA: Probabilistic Principal Component Analysis PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA feels algebraic — where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the maximum-likelihood solution of a simple linear Gaussian latent-variable model. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear + Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.\nBelow I present a single self-contained story: model → marginal → posterior → likelihood → MLE solution → interpretation. I’ll highlight the key identities you need and keep the algebra explicit so nothing mysterious is swept under the rug.\nMotivation We want a generative model for $d$-dimensional observations $t$ that:\nIs simple enough to solve analytically; and Reduces to classical PCA in an appropriate limit. A latent-variable model does exactly this: introduce a low-dimensional latent $x\\in\\mathbb{R}^q$ ($q",
  "wordCount" : "1775",
  "inLanguage": "en",
  "datePublished": "2025-10-20T12:20:23-05:00",
  "dateModified": "2025-10-20T12:20:23-05:00",
  "author":{
    "@type": "Person",
    "name": "Kenyi Takagui-Perez"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://taogenna.github.io/kenyi-blog/docs/ppca/ppca/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Kenyi'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://taogenna.github.io/kenyi-blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://taogenna.github.io/kenyi-blog/" accesskey="h" title="Kenyi&#39;Log (Alt + H)">Kenyi&#39;Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://taogenna.github.io/kenyi-blog/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://taogenna.github.io/kenyi-blog/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://taogenna.github.io/kenyi-blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://taogenna.github.io/kenyi-blog/bookshelf/" title="Bookshelf">
                    <span>Bookshelf</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      PPCA: The Minimal Generative Model
    </h1>
    <div class="post-meta"><span title='2025-10-20 12:20:23 -0500 -0500'>October 20, 2025</span>&nbsp;·&nbsp;Kenyi Takagui-Perez

</div>
  </header> 
  <div class="post-content"><h1 id="ppca-probabilistic-principal-component-analysis">PPCA: Probabilistic Principal Component Analysis<a hidden class="anchor" aria-hidden="true" href="#ppca-probabilistic-principal-component-analysis">#</a></h1>
<p>PCA is one of those algorithms you learn early and then treat as a black box: center the data, compute the top eigenvectors of the sample covariance, project. But PCA <em>feels</em> algebraic — where does it come from probabilistically? Probabilistic PCA (PPCA) answers that question: it is the <strong>maximum-likelihood solution of a simple linear Gaussian latent-variable model</strong>. Understanding PPCA is a tiny, high-value step on the path to modern generative models (VAEs, normalizing flows): it isolates the linear + Gaussian case where every calculation is analytic, so you can see clearly how inference and likelihood tie together.</p>
<p>Below I present a single self-contained story: model → marginal → posterior → likelihood → MLE solution → interpretation. I’ll highlight the key identities you need and keep the algebra explicit so nothing mysterious is swept under the rug.</p>
<hr>
<h1 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h1>
<p>We want a generative model for $d$-dimensional observations $t$ that:</p>
<ul>
<li>Is simple enough to solve analytically; and</li>
<li>Reduces to classical PCA in an appropriate limit.</li>
</ul>
<p>A latent-variable model does exactly this: introduce a low-dimensional latent $x\in\mathbb{R}^q$ ($q&lt;d$) and pick simple Gaussian forms so all integrals and conditionals are Gaussian.</p>
<hr>
<h1 id="model-linear-gaussian-latent-variable">Model (linear Gaussian latent variable)<a hidden class="anchor" aria-hidden="true" href="#model-linear-gaussian-latent-variable">#</a></h1>
<p>Assume the generative process
$$
x \sim \mathcal{N}(0, I_q),\qquad
t \mid x \sim \mathcal{N}(W x + \mu,, \sigma^2 I_d),
$$
where $W$ is a $d\times q$ matrix (the linear mapping), $\mu\in\mathbb{R}^d$ is the mean, and $\sigma^2&gt;0$ is isotropic noise variance. This is the PPCA model (Tipping &amp; Bishop, 1999).</p>
<p>Two immediate facts:</p>
<ol>
<li>Because both prior and likelihood are Gaussian, the marginal $p(t)$ and posterior $p(x\mid t)$ are Gaussian and computable in closed form.</li>
<li>The model has a rotational ambiguity in $W$: replacing $W$ by $W R$ for any orthogonal $R\in\mathbb{R}^{q\times q}$ leaves the model unchanged (with appropriate change of latent coordinates). So $W$ is identifiable only up to orthogonal transforms.</li>
</ol>
<p>For the sake of clarity I&rsquo;ll use the explanation of <a href="https://medium.com/practical-coding/the-simplest-generative-model-you-probably-missed-c840d68b704">Oliver</a>:
<figure class="align-center">
    <img loading="lazy" src="../ppca_prince.jpg"
         alt="Description of the image"/> <figcaption>
            <p>Bishop’s “Pattern Recognition and Machine Learning”, chapter 12.</p>
        </figcaption>
</figure>
</p>
<ul>
<li>Left panel: After sampling a variable from the latent distribution,</li>
<li>Middle panel: The visibles are drawn from an isotropic Gaussian (diagonal covariance matrix) around $W * x_h + \mu$.</li>
<li>Right panel: The resulting marginal distribution for the observables is also a Gaussian, but not isotropic.</li>
</ul>
<hr>
<h1 id="marginal-distribution-pt">Marginal distribution $p(t)$<a hidden class="anchor" aria-hidden="true" href="#marginal-distribution-pt">#</a></h1>
<p>We compute the marginal by integrating out $x$:
$$
p(t) = \int p(t\mid x),p(x),dx.
$$
Set $r := t-\mu$. Using the Gaussian forms,
$$
p(t\mid x),p(x)
\propto
\exp\Big(-\tfrac{1}{2}\big[(r - W x)^\top (\sigma^{-2} I_d)(r - W x) + x^\top x\big]\Big).
$$
Collect the quadratic terms in $x$:</p>
<p>$$
\frac{1}{\sigma^{2}}(r - W x)^{T}(r - W x) + x^{T}x = \frac{1}{\sigma^{2}} r^{T}r - \frac{2}{\sigma^{2}} x^{T} W^{T} r + x^{T}\Big(I_q + \frac{1}{\sigma^{2}} W^{T} W\Big)x.
$$</p>
<p>Define the $q\times q$ matrix
$$
M := I_q + \frac{1}{\sigma^2} W^{T}W.
$$
Completing the square in $x$ gives a Gaussian integral whose value is $(2\pi)^{q/2} |M|^{-1/2}$. Carefully tracking normalization constants (from $p(x)$ and $p(t\mid x)$) yields
$$
p(t)
= \frac{1}{(2\pi)^{d/2} (\sigma^2)^{d/2} } |M|^{-1/2}
\exp\Big(-\tfrac{1}{2} r^\top \big(\tfrac{1}{\sigma^2} I_d - \tfrac{1}{\sigma^4} W M^{-1} W^\top\big) r\Big).
$$</p>
<p>Now recognize the familiar covariance and precision using two matrix identities.</p>
<h2 id="woodbury-identity-precision">Woodbury identity (precision)<a hidden class="anchor" aria-hidden="true" href="#woodbury-identity-precision">#</a></h2>
<p>For $C := W W^\top + \sigma^2 I_d$,
$$
C^{-1} = (\sigma^2 I_d + W W^\top)^{-1}
= \frac{1}{\sigma^2} I_d - \frac{1}{\sigma^4} W\big(I_q + \tfrac{1}{\sigma^2}W^\top W\big)^{-1}W^\top
= \frac{1}{\sigma^2} I_d - \frac{1}{\sigma^4} W M^{-1} W^\top.
$$
This matches the precision we found in the exponent.</p>
<h2 id="matrix-determinant-lemma">Matrix determinant lemma<a hidden class="anchor" aria-hidden="true" href="#matrix-determinant-lemma">#</a></h2>
<p>$$
|W W^\top + \sigma^2 I_d| = (\sigma^2)^d,\Big|I_q + \frac{1}{\sigma^2}W^\top W\Big| = (\sigma^2)^d |M|.
$$
Therefore $(\sigma^2)^{d/2}|M|^{1/2}=|C|^{1/2}$.</p>
<p>Putting the pieces together,
$$
\boxed{p(t) = \mathcal{N}(t\mid \mu,, C), \qquad
C := W W^\top + \sigma^2 I_d. }
$$</p>
<p>So PPCA is a Gaussian density with covariance equal to the low-rank part $W W^\top$ plus isotropic noise.</p>
<hr>
<h1 id="posterior-pxmid-t-exact-inference">Posterior $p(x\mid t)$ (exact inference)<a hidden class="anchor" aria-hidden="true" href="#posterior-pxmid-t-exact-inference">#</a></h1>
<p>From the quadratic completion above one also reads the posterior over $x$:
$$
p(x\mid t) = \mathcal{N}\big(x \mid m, \Sigma\big)
$$
with
$$
\boxed{ \Sigma = \sigma^2 \big(W^\top W + \sigma^2 I_q\big)^{-1}, \qquad
m = \big(W^\top W + \sigma^2 I_q\big)^{-1} W^\top (t-\mu).}
$$
(Equivalently, define $M := I_q + \sigma^{-2}W^\top W$ and you get $\Sigma=M^{-1}$ and $m = M^{-1}\sigma^{-2}W^\top (t-\mu)$; both forms are algebraically the same.)</p>
<p>Two intuitions:</p>
<ul>
<li>When $\sigma^2$ is small, posterior variance is small and $m\approx (W^\top W)^{-1}W^\top (t-\mu)$, i.e. classical least-squares projection onto the column space of $W$.</li>
<li>When $\sigma^2$ is large, the posterior concentrates near zero (the prior) because the observation is noisy.</li>
</ul>
<hr>
<h1 id="likelihood-for-a-dataset-and-maximum-likelihood-estimation">Likelihood for a dataset and maximum likelihood estimation<a hidden class="anchor" aria-hidden="true" href="#likelihood-for-a-dataset-and-maximum-likelihood-estimation">#</a></h1>
<p>Given $N$ i.i.d. datapoints ${t_i}_{i=1}^N$,

$$
\begin{align*}
p(T | W,\psi, \mu) &= \prod_{i=1}^N p(t_i | W,\psi, \mu)\\
&= (2\pi)^{-ND/2}|C|^{-N/2}\exp{\Big\{ -\frac{1}{2}\sum_{i=1}^N (t_i-\mu)^TC^{-1}(t_i-\mu) \Big\}}
\end{align*}
$$

the log-likelihood is
$$
\log p(T\mid W,\sigma^2,\mu)
= -\frac{N}{2}\Big( d\log(2\pi) + \log|C| + \operatorname{Tr}\big(C^{-1} S\big) \Big),
$$
where $S$ is the sample covariance

$$
S := \frac{1}{N}\sum_{i=1}^N (t_i-\mu)(t_i-\mu)^\top.
$$

For fixed $\mu$ (choose $\mu=\bar t$), maximizing the likelihood reduces to minimizing
$$
L(W,\sigma^2) =\log|C| + \operatorname{Tr}(C^{-1} S), \qquad C = W W^\top + \sigma^2 I_d.
$$</p>
<p>Tipping &amp; Bishop show that the stationary point can be written in closed form using the top eigenstructure of $S$. Let the eigen-decomposition be
$$
S = U \Lambda U^\top,\qquad \Lambda = \operatorname{diag}(\lambda_1,\dots,\lambda_d),\quad \lambda_1\ge\cdots\ge\lambda_d.
$$
Split $U = [U_q\ U_{d-q}]$ and $\Lambda = \operatorname{diag}(\Lambda_q,\Lambda_{d-q})$ where $U_q$ are the top $q$ eigenvectors and $\Lambda_q$ the corresponding eigenvalues. Then the maximum-likelihood estimates are</p>

$$

\boxed{
\begin{aligned}
\mu^* &= \bar t,\\[4pt]
W^* &= U_q\big(\Lambda_q - \sigma^{2*} I_q\big)^{1/2} R,\qquad R\in\mathbb{R}^{q\times q}\ \text{orthogonal},\\[4pt]
\sigma^{2*} &= \frac{1}{d-q}\sum_{j=q+1}^{d}\lambda_j.
\end{aligned}
}
$$

<p>A few remarks:</p>
<ul>
<li>The orthogonal matrix $R$ remains free: it is the rotational indeterminacy of the latent coordinates.</li>
<li>The noise variance $\sigma^{2*}$ is the average of the discarded eigenvalues (the variance not explained by the principal subspace).</li>
<li>When $\sigma^2\to 0$, the columns of $W^*$ span the same subspace as the top-$q$ PCA directions and $W^*W^{*\top}$ tends to $U_q\Lambda_q U_q^\top$. Thus PPCA recovers classical PCA as a limiting case.</li>
</ul>
<p>If you want to compute the MLE in practice:</p>
<ul>
<li>You can directly solve for $\sigma^{2*}$ from the eigenvalues and then compute $W^*$ from the top eigenpairs (this is Tipping &amp; Bishop&rsquo;s closed form).</li>
<li>Alternatively, you can optimize $L(W,\sigma^2)$ numerically or use an EM algorithm (below) if you prefer an iterative approach that cleanly separates the inference (E-step) and parameter update (M-step).</li>
</ul>
<hr>
<h1 id="em-algorithm">EM algorithm<a hidden class="anchor" aria-hidden="true" href="#em-algorithm">#</a></h1>
<p>EM treats $x$ as missing data. With current parameters $(W,\sigma^2)$ compute posterior expectations
$$
\mathbb{E}[x\mid t] = m,\qquad \mathbb{E}[x x^\top \mid t] = \Sigma + m m^\top.
$$
Then M-step updates $W$ and $\sigma^2$ by solving linear equations involving these expectations (closed form). EM is useful when you want a monotonic increase in likelihood or when you want to generalize (e.g. to non-isotropic noise $\Psi$) where closed forms are less tidy.</p>
<hr>
<h1 id="ppca-vs-factor-analysis-vs-pca">PPCA vs Factor Analysis vs PCA<a hidden class="anchor" aria-hidden="true" href="#ppca-vs-factor-analysis-vs-pca">#</a></h1>
<ul>
<li><strong>Classical PCA</strong>: algorithmic; finds subspace spanned by top eigenvectors of $S$. No generative likelihood (no noise model).</li>
<li><strong>PPCA</strong>: linear generative model with <em>isotropic</em> Gaussian noise $\sigma^2 I_d$. Likelihood is available and PCA arises in the limit $\sigma^2\to 0$.</li>
<li><strong>Factor analysis (FA)</strong>: same linear structure but with <em>diagonal</em> noise covariance $\Psi$ (not necessarily isotropic). FA is more flexible but algebraically messier — closed-form MLEs for $\Psi$ do not generally exist and EM is standard.</li>
</ul>
<hr>
<h1 id="sampling--use-as-a-generative-model">Sampling &amp; use as a generative model<a hidden class="anchor" aria-hidden="true" href="#sampling--use-as-a-generative-model">#</a></h1>
<p>Sampling from PPCA is trivial and instructive:</p>
<ol>
<li>Sample $x\sim\mathcal{N}(0,I_q)$.</li>
<li>Sample noise $\epsilon\sim\mathcal{N}(0,\sigma^2 I_d)$.</li>
<li>Return $t = W x + \mu + \epsilon$.</li>
</ol>
<p>This generates samples from $\mathcal{N}(\mu, W W^\top + \sigma^2 I_d)$. So PPCA is <em>not</em> a complex high-capacity generative model, but it provides a clear probabilistic story for projection + reconstruction and clarifies the role of latent dimensions and noise.</p>
<h1 id="code-and-visuals">Code and visuals<a hidden class="anchor" aria-hidden="true" href="#code-and-visuals">#</a></h1>
<figure class="align-center">
    <img loading="lazy" src="../ppca_output.jpg"
         alt="Description of the image"/> <figcaption>
            <p>We start from the observed data (in blue) and through the PPCA max log likelihood we find the optimized parameters, which once plugged in the linear relation and sample some values from the latent space we effectively generate a new dataset (in red) with similar distribution as the observed dataset.</p>
        </figcaption>
</figure>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> numpy.linalg <span style="color:#f92672">import</span> eigh
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 1. Generate some real 2D data</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>N <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>z <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(N, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>T_real <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>hstack([z, <span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>z <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(N, <span style="color:#ae81ff">1</span>)])  <span style="color:#75715e"># correlated data</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 2. Estimate PPCA parameters</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Center data</span>
</span></span><span style="display:flex;"><span>mu <span style="color:#f92672">=</span> T_real<span style="color:#f92672">.</span>mean(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>X_centered <span style="color:#f92672">=</span> T_real <span style="color:#f92672">-</span> mu
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sample covariance</span>
</span></span><span style="display:flex;"><span>S <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cov(X_centered<span style="color:#f92672">.</span>T)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Eigen decomposition</span>
</span></span><span style="display:flex;"><span>vals, vecs <span style="color:#f92672">=</span> eigh(S)
</span></span><span style="display:flex;"><span>idx <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argsort(vals)[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>vals, vecs <span style="color:#f92672">=</span> vals[idx], vecs[:, idx]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Latent dimension q = 1</span>
</span></span><span style="display:flex;"><span>q <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>U_q <span style="color:#f92672">=</span> vecs[:, :q]
</span></span><span style="display:flex;"><span>Lambda_q <span style="color:#f92672">=</span> vals[:q]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute sigma^2 and W</span>
</span></span><span style="display:flex;"><span>D <span style="color:#f92672">=</span> T_real<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>sigma2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(vals[q:])  <span style="color:#75715e"># average of remaining eigenvalues</span>
</span></span><span style="display:flex;"><span>W <span style="color:#f92672">=</span> U_q <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(Lambda_q <span style="color:#f92672">-</span> sigma2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Learned W =</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>W<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Learned sigma^2 = </span><span style="color:#e6db74">{</span>sigma2<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Mean = </span><span style="color:#e6db74">{</span>mu<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 3. Generate new data from model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span>N_gen <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>x_gen <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(N_gen, q)
</span></span><span style="display:flex;"><span>eps <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(sigma2) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(N_gen, D)
</span></span><span style="display:flex;"><span>T_gen <span style="color:#f92672">=</span> x_gen <span style="color:#f92672">@</span> W<span style="color:#f92672">.</span>T <span style="color:#f92672">+</span> mu <span style="color:#f92672">+</span> eps
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 4. Compare</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------------------------------</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(T_real[:,<span style="color:#ae81ff">0</span>], T_real[:,<span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Real data&#34;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;royalblue&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(T_gen[:,<span style="color:#ae81ff">0</span>], T_gen[:,<span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Generated from PPCA&#34;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tomato&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;PPCA as a Generative Model&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;equal&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><hr>
<h1 id="intuition--how-to-read-the-equations">Intuition &amp; how to read the equations<a hidden class="anchor" aria-hidden="true" href="#intuition--how-to-read-the-equations">#</a></h1>
<ul>
<li>$W W^\top$ captures the <strong>structured, low-rank variability</strong> in the data (the principal subspace).</li>
<li>$\sigma^2 I_d$ captures <strong>isotropic residual noise</strong> — variance not explained by the subspace.</li>
<li>The posterior $p(x\mid t)$ tells you how to infer latent coordinates of a datum: as noise decreases ($\sigma^2\to0$) the posterior collapses to a point and the mean becomes the usual linear projection onto principal components.</li>
<li>The MLE for $\sigma^2$ being the mean of the discarded eigenvalues gives a neat decomposition: total variance = explained (top $q$ eigenvalues) + unexplained (average of remaining eigenvalues).</li>
</ul>
<hr>
<h1 id="bridge-to-vaes-why-study-ppca-first">Bridge to VAEs (why study PPCA first)<a hidden class="anchor" aria-hidden="true" href="#bridge-to-vaes-why-study-ppca-first">#</a></h1>
<p>A Variational Autoencoder (VAE) generalizes PPCA in two directions:</p>
<ul>
<li>Replace the linear decoder $W x + \mu$ by a neural network $f_\theta(x)$ (nonlinear decoder).</li>
<li>Replace exact Gaussian posterior inference (available in PPCA) with an <em>approximate</em> amortized posterior $q_\phi(x\mid t)$ (the encoder), trained by variational inference.</li>
</ul>
<p>PPCA is therefore the canonical linear Gaussian toy model where inference, sampling, and MLE are closed form. Studying PPCA first gives you a clean map of what changes when you make the model nonlinear and inference approximate.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] M. E. Tipping and C. M. Bishop, “Probabilistic Principal Component Analysis,” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61, 611–622 (1999), <a href="https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf">https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf</a></p>
<p>[2] S. Patel, “The Simplest Generative Model You Probably Missed,” Medium (2018), <a href="https://medium.com/practical-coding/the-simplest-generative-model-you-probably-missed-c840d68b704">https://medium.com/practical-coding/the-simplest-generative-model-you-probably-missed-c840d68b704</a></p>
<p>[3] S. J. D. Prince, <em>Understanding Deep Learning</em> (MIT Press, 2023), <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a></p>
<h1 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h1>
<p>Cited as:</p>
<blockquote>
<p>Takagui-Perez, R. Kenyi. PPCA: The Minimal Generative Model, Kenyi&rsquo;Log.<br>
<a href="https://taogenna.github.io/kenyi-blog/docs/PPCA/PPCA/">https://taogenna.github.io/kenyi-blog/docs/PPCA/PPCA/</a></p></blockquote>
<p>Or</p>
<pre tabindex="0"><code>@article{something,
  title   = &#34;PPCA: The Minimal Generative Model&#34;,
  author  = &#34;Takagui-Perez, R. Kenyi&#34;,
  journal = &#34;https://taogenna.github.io/kenyi-blog/&#34;,
  year    = &#34;2025&#34;,
  month   = &#34;Oct&#34;,
  url     = &#34;https://taogenna.github.io/kenyi-blog/docs/PPCA/PPCA/&#34;
}
</code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on x"
            href="https://x.com/intent/tweet/?text=PPCA%3a%20The%20Minimal%20Generative%20Model&amp;url=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f&amp;title=PPCA%3a%20The%20Minimal%20Generative%20Model&amp;summary=PPCA%3a%20The%20Minimal%20Generative%20Model&amp;source=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f&title=PPCA%3a%20The%20Minimal%20Generative%20Model">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on whatsapp"
            href="https://api.whatsapp.com/send?text=PPCA%3a%20The%20Minimal%20Generative%20Model%20-%20https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on telegram"
            href="https://telegram.me/share/url?text=PPCA%3a%20The%20Minimal%20Generative%20Model&amp;url=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PPCA: The Minimal Generative Model on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=PPCA%3a%20The%20Minimal%20Generative%20Model&u=https%3a%2f%2ftaogenna.github.io%2fkenyi-blog%2fdocs%2fppca%2fppca%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://taogenna.github.io/kenyi-blog/">Kenyi&#39;Log</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
